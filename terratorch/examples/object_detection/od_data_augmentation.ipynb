{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08040bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.datasets.od_augmentation import CopyPasteObjectDetectionDataset\n",
    "from terratorch.datasets.generic_od_dataset import GenericObjectDetectionDataset\n",
    "import os\n",
    "image_dir = os.environ.get(\"image_dir\", \"/home/romeokienzler/Downloads/swisstopo/\")\n",
    "object_folder = os.environ.get(\"object_folder\", \"/home/romeokienzler/Downloads/objects/\")\n",
    "tile_cache_dir = os.environ.get(\"tile_cache_dir\", \"/home/romeokienzler/Downloads/swisstopo_tile_cache/\")\n",
    "checkpoint_path = os.environ.get('checkpoint_path', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7e8077",
   "metadata": {},
   "source": [
    "# Object Detection Data Augmentation with Copy-Paste\n",
    "\n",
    "This notebook demonstrates how to use the Copy-Paste augmentation technique for object detection datasets.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "For this notebook to work with real data, you need:\n",
    "\n",
    "1. **Images**: A folder of images (JPG, PNG, TIFF, etc.)\n",
    "2. **Annotations**: YOLO format `.txt` files with the same name as images, containing:\n",
    "   ```\n",
    "   <class_id> <center_x_norm> <center_y_norm> <width_norm> <height_norm>\n",
    "   ```\n",
    "   where normalized coordinates are in range [0, 1], one line per object per image.\n",
    "\n",
    "3. **Objects for pasting**: RGBA PNG images in the `object_folder`\n",
    "\n",
    "**Note**: The environment variables (`image_dir`, `object_folder`, `tile_cache_dir`) need to point to directories with actual annotated data for training to work. Without annotation files, the dataset will be empty and training will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa470c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GenericObjectDetectionDataset(\n",
    "    image_dir=image_dir,\n",
    ")\n",
    "\n",
    "sample = dataset[0]\n",
    "print(sample[\"image\"].shape)\n",
    "print(sample[\"boxes\"].shape)\n",
    "print(sample[\"labels\"].shape)\n",
    "print(sample[\"boxes\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16fd1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.datasets.od_tiled_dataset_wrapper import TiledDataset\n",
    "tiled_dataset = TiledDataset(\n",
    "    base_dataset=dataset,\n",
    "    tile_size=(512, 512),\n",
    "    overlap=0,\n",
    "    cache_dir=tile_cache_dir,\n",
    "    skip_empty_boxes=False, # we dont have any boxes yet as we add them later via augmentation\n",
    "    rebuild=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3665a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_aug = CopyPasteObjectDetectionDataset(\n",
    "    base_dataset=tiled_dataset,\n",
    "    object_folder=object_folder,\n",
    "    scale_range=(0.01, 0.01),\n",
    "    max_objects=3,\n",
    "    paste_prob=1\n",
    ")\n",
    "\n",
    "sample = dataset_aug[0]\n",
    "print(sample[\"image\"].shape, sample[\"mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae15d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot original dataset image with boxes and labels\n",
    "import terratorch.visualization as ttv\n",
    "sample_original = dataset[0]\n",
    "ttv.plot_boxes_labels(sample[\"image\"], sample[\"boxes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a14d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import lightning as L\n",
    "import torch\n",
    "\n",
    "class GenericDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset=None,\n",
    "        train_dataset=None,\n",
    "        val_dataset=None,\n",
    "        test_dataset=None,\n",
    "        predict_dataset=None,\n",
    "        split_ratio=(0.6, 0.2, 0.2),\n",
    "        seed=42,\n",
    "        batch_size=1,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        collate_fn=None,\n",
    "        shuffle_train=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.predict_dataset = predict_dataset\n",
    "\n",
    "        self.split_ratio = split_ratio\n",
    "        self.seed = seed\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.persistent_workers = persistent_workers\n",
    "        self.collate_fn = collate_fn\n",
    "        self.shuffle_train = shuffle_train\n",
    "\n",
    "        if dataset is None and train_dataset is None:\n",
    "            raise ValueError(\n",
    "                \"You must provide either `dataset` or `train_dataset`.\"\n",
    "            )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # already explicitly provided → do nothing\n",
    "        if self.train_dataset is not None:\n",
    "            return\n",
    "\n",
    "        if self.dataset is None:\n",
    "            raise ValueError(\"Dataset is None but no explicit splits were provided.\")\n",
    "\n",
    "        n = len(self.dataset)\n",
    "        r_train, r_val, r_test = self.split_ratio\n",
    "\n",
    "        if not abs(r_train + r_val + r_test - 1.0) < 1e-6:\n",
    "            raise ValueError(f\"split_ratio must sum to 1, got {self.split_ratio}\")\n",
    "\n",
    "        n_train = int(n * r_train)\n",
    "        n_val = int(n * r_val)\n",
    "        n_test = n - n_train - n_val  # remainder → test\n",
    "\n",
    "        g = torch.Generator().manual_seed(self.seed)\n",
    "\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
    "            self.dataset,\n",
    "            [n_train, n_val, n_test],\n",
    "            generator=g\n",
    "        )\n",
    "\n",
    "    def _loader(self, dataset, shuffle=False):\n",
    "        if dataset is None:\n",
    "            return None\n",
    "\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            persistent_workers=self.persistent_workers and self.num_workers > 0,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self._loader(self.train_dataset, shuffle=self.shuffle_train)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._loader(self.val_dataset, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._loader(self.test_dataset, shuffle=False)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return self._loader(self.predict_dataset, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2441261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_collate(batch):\n",
    "    images = torch.stack([b[\"image\"] for b in batch])  # [B, C, H, W]\n",
    "\n",
    "    targets = {\n",
    "        \"boxes\": [b[\"boxes\"] for b in batch],\n",
    "        \"labels\": [b[\"labels\"] for b in batch],\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"image\": images,\n",
    "        **targets,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "dm = GenericDataModule(\n",
    "    dataset=dataset_aug,\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    collate_fn=detection_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d08b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc58bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dm.train_dataloader()\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf182d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.tasks import ObjectDetectionTask\n",
    "from terratorch.models.object_detection_model_factory import ObjectDetectionModelFactory\n",
    "\n",
    "model = ObjectDetectionTask(\n",
    "    model_factory=\"ObjectDetectionModelFactory\",\n",
    "    model_args={\n",
    "        \"framework\": \"faster-rcnn\",\n",
    "        \"backbone\": \"terramind_v1_tiny\",\n",
    "        \"backbone_pretrained\": True,\n",
    "        \"num_classes\": 2,\n",
    "        \"framework_min_size\": 512,\n",
    "        \"framework_max_size\": 512,\n",
    "        \"backbone_modalities\": [\"RGB\"],\n",
    "        \"in_channels\": 3,\n",
    "        \"necks\": [\n",
    "            {\n",
    "                \"name\": \"SelectIndices\",\n",
    "                \"indices\": [2, 5, 8, 11],\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"ReshapeTokensToImage\",\n",
    "                \"remove_cls_token\": False,\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"LearnedInterpolateToPyramidal\",\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"FeaturePyramidNetworkNeck\",\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    freeze_backbone=False,\n",
    "    freeze_decoder=False,\n",
    "    class_names=[\n",
    "        \"Background\",\n",
    "        \"SimpleObject\",\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Subset\n",
    "import terratorch.visualization as ttv\n",
    "\n",
    "\n",
    "class LogDetectionGrid(pl.Callback):\n",
    "    def __init__(self, grid_size: int = 4, every_n_epochs: int = 1, score_thr=0.3):\n",
    "        self.grid_size = grid_size\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "        self.score_thr = score_thr\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        epoch = trainer.current_epoch\n",
    "        if epoch % self.every_n_epochs != 0:\n",
    "            return\n",
    "\n",
    "        pl_module.eval()\n",
    "\n",
    "        loader = trainer.datamodule.val_dataloader()\n",
    "        batch = next(iter(loader))\n",
    "\n",
    "        images = batch[\"image\"].to(pl_module.device)\n",
    "\n",
    "        outputs = pl_module(images)\n",
    "        preds = outputs.predictions  # <-- KEY FIX\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            self.grid_size,\n",
    "            self.grid_size,\n",
    "            figsize=(self.grid_size * 4, self.grid_size * 4),\n",
    "            tight_layout=True,\n",
    "        )\n",
    "\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i >= len(preds):\n",
    "                break\n",
    "\n",
    "            pred = preds[i]\n",
    "            keep = pred[\"scores\"] > self.score_thr\n",
    "\n",
    "            ttv.plot_boxes_labels(\n",
    "                image=images[i].cpu(),\n",
    "                boxes=pred[\"boxes\"][keep].cpu(),\n",
    "                labels=pred[\"labels\"][keep].cpu(),\n",
    "                scores=pred[\"scores\"][keep].cpu(),\n",
    "                ax=ax,\n",
    "                show=False,\n",
    "            )\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        trainer.logger.experiment.add_figure(\n",
    "            \"val/predictions\",\n",
    "            fig,\n",
    "            global_step=epoch,\n",
    "        )\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f05186",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[LogDetectionGrid(grid_size=4)],)\n",
    "\n",
    "if checkpoint_path:\n",
    "    # Resume training from checkpoint\n",
    "    trainer.fit(model, dm, ckpt_path=checkpoint_path)\n",
    "else:\n",
    "    # Train from scratch\n",
    "    trainer.fit(model, dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
