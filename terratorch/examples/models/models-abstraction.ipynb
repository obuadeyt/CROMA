{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4d8312b",
   "metadata": {},
   "source": [
    "# TerraTorch: 4 Levels of Abstraction to Use Terratorch's Model Registry in a Notebook\n",
    "\n",
    "This notebook teaches you how to use TerraTorch models at different levels:\n",
    "\n",
    "| Level | What You Get | Method to Call | When to Use |\n",
    "|-------|--------------|----------------|-------------|\n",
    "| 1. Backbone | Feature extractor only | `BACKBONE_REGISTRY.build(\"model_name\")` | Research, custom pipelines |\n",
    "| 2. Full Model | Backbone + Decoder + Head | `EncoderDecoderFactory().build_model(...)` | Inference |\n",
    "| 3. Task | Model + training logic | `SemanticSegmentationTask(...)` | Custom training loops |\n",
    "| 4. Task + DataModule + Trainer | Complete pipeline | `Trainer.fit(task, datamodule)` | Full training runs |\n",
    "\n",
    "**Key files to study:**\n",
    "- Level 1: `terratorch/registry/registry.py`\n",
    "- Level 2: `terratorch/models/encoder_decoder_factory.py`\n",
    "- Level 3: `terratorch/tasks/base_task.py`\n",
    "- Level 4: `terratorch/datamodules/generic_pixel_wise_data_module.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1c4aa7",
   "metadata": {},
   "source": [
    "## Level 1: Backbone Only\n",
    "\n",
    "A **backbone** is a neural network that extracts features from images. It doesn't make predictions ‚Äî it just \"sees\" patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f95e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 1: Load a backbone\n",
    "from terratorch.registry import BACKBONE_REGISTRY\n",
    "import torch\n",
    "\n",
    "# Build a backbone\n",
    "backbone = BACKBONE_REGISTRY.build(\"prithvi_eo_v2_100_tl\", pretrained=True)\n",
    "print(f\"Type: {type(backbone).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca8ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how the backbone processes an image\n",
    "fake_image = torch.randn(1, 6, 224, 224)  # [batch, channels, height, width]\n",
    "\n",
    "backbone.eval()\n",
    "with torch.no_grad():\n",
    "    features = backbone(fake_image)\n",
    "\n",
    "# Print output shapes\n",
    "print(\"Features shape:\")\n",
    "for i, f in enumerate(features):\n",
    "    print(f\"  tensor {i}: {f.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b1f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backbone Families Summary\n",
    "print(\"\\nüèóÔ∏è TERRATORCH BACKBONE FAMILIES\\n\")\n",
    "print(f\"{'Family':<35} {'Count':<10} {'Example Models'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for source_name, source in BACKBONE_REGISTRY._sources.items():\n",
    "    try:\n",
    "        models = list(source)\n",
    "        examples = \", \".join(models[:3])\n",
    "        if len(models) > 3:\n",
    "            examples += \"...\"\n",
    "        print(f\"{source_name:<35} {len(models):<10} {examples}\")\n",
    "    except:\n",
    "        print(f\"{source_name:<35} {'dynamic':<10} (loaded on demand)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc209919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TerraTorch native models (geospatial-focused)\n",
    "print(\"üõ∞Ô∏è TERRATORCH MODELS (Geospatial)\")\n",
    "print(\"-\" * 40)\n",
    "terratorch_models = list(BACKBONE_REGISTRY._sources['terratorch'])\n",
    "for m in sorted(terratorch_models)[:15]:\n",
    "    print(f\"  ‚Ä¢ {m}\")\n",
    "print(f\"  ... Total: {len(terratorch_models)}\")\n",
    "\n",
    "# Timm models (general vision)\n",
    "print(\"\\nüì∑ TIMM MODELS (General Vision)\")\n",
    "print(\"-\" * 40)\n",
    "timm_models = list(BACKBONE_REGISTRY._sources['timm'])\n",
    "for m in sorted(timm_models)[:15]:\n",
    "    print(f\"  ‚Ä¢ {m}\")\n",
    "print(f\"  ... Total: {len(timm_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec0ad45",
   "metadata": {},
   "source": [
    "## Level 2: Full Model (Backbone + Decoder + Head)\n",
    "\n",
    "A **full model** combines:\n",
    "- **Backbone**: Extracts features\n",
    "- **Decoder**: Transforms features (e.g., upsamples)\n",
    "- **Head**: Makes predictions\n",
    "\n",
    "Now we can get actual outputs (e.g., segmentation masks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43254e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 2: Build a full model\n",
    "from terratorch.models import EncoderDecoderFactory\n",
    "from terratorch.datasets import HLSBands\n",
    "import torch\n",
    "\n",
    "# Create fake image for testing\n",
    "fake_image = torch.randn(1, 6, 224, 224)  # [batch, channels, height, width]\n",
    "\n",
    "factory = EncoderDecoderFactory()\n",
    "model = factory.build_model(\n",
    "    task=\"segmentation\",\n",
    "    backbone=\"prithvi_eo_v2_100_tl\",\n",
    "    decoder=\"UperNetDecoder\",\n",
    "    backbone_bands=[HLSBands.BLUE, HLSBands.GREEN, HLSBands.RED,\n",
    "                    HLSBands.NIR_NARROW, HLSBands.SWIR_1, HLSBands.SWIR_2],\n",
    "    num_classes=5,\n",
    "    backbone_pretrained=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef66b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the full model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(fake_image)\n",
    "\n",
    "print(f\"Input: {fake_image.shape}\")\n",
    "print(f\"Output: {output.output.shape}\")  # [batch, num_classes, H, W]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09bd756",
   "metadata": {},
   "source": [
    "## Level 3: Task Class\n",
    "\n",
    "A **Task** wraps the model with training logic:\n",
    "- Loss function\n",
    "- Metrics (accuracy, mAP, etc.)\n",
    "- `training_step()`, `validation_step()`\n",
    "\n",
    "It's a PyTorch Lightning module ‚Äî ready for training but needs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdfc7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 3: Create a Task\n",
    "from terratorch.tasks import SemanticSegmentationTask\n",
    "\n",
    "task = SemanticSegmentationTask(\n",
    "    model_args={\n",
    "        \"backbone\": \"prithvi_eo_v2_100_tl\",\n",
    "        \"decoder\": \"UperNetDecoder\",\n",
    "        \"num_classes\": 5,\n",
    "        \"backbone_pretrained\": True,\n",
    "    },\n",
    "    loss=\"ce\",\n",
    "    model_factory=\"EncoderDecoderFactory\",\n",
    ")\n",
    "\n",
    "print(f\"Task type: {type(task).__name__}\")\n",
    "print(f\"Model inside: {type(task.model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd3845",
   "metadata": {},
   "source": [
    "## Level 4: Task + DataModule + Trainer\n",
    "\n",
    "A **DataModule** handles data loading (train/val/test splits, transforms, batching).\n",
    "\n",
    "A **Trainer** runs the training loop.\n",
    "\n",
    "Together: complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ef644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 4: Working example with fake data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lightning import Trainer\n",
    "import torch\n",
    "\n",
    "# Create a fake dataset\n",
    "class FakeSegmentationDataset(Dataset):\n",
    "    def __init__(self, num_samples=4):\n",
    "        self.num_samples = num_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Fake image: 6 channels, 224x224\n",
    "        image = torch.randn(6, 224, 224)\n",
    "        # Fake mask: class labels 0-4 for each pixel\n",
    "        mask = torch.randint(0, 5, (224, 224))\n",
    "        return {\"image\": image, \"mask\": mask}\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataset = FakeSegmentationDataset(num_samples=4)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2)\n",
    "\n",
    "val_dataset = FakeSegmentationDataset(num_samples=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "# Create a fresh task\n",
    "from terratorch.tasks import SemanticSegmentationTask\n",
    "\n",
    "task = SemanticSegmentationTask(\n",
    "    model_args={\n",
    "        \"backbone\": \"prithvi_eo_v2_100_tl\",\n",
    "        \"decoder\": \"UperNetDecoder\",\n",
    "        \"num_classes\": 5,\n",
    "        \"backbone_pretrained\": True,\n",
    "    },\n",
    "    loss=\"ce\",\n",
    "    ignore_index=-1,  # Required: index to ignore in loss (-1 = none ignored)\n",
    "    model_factory=\"EncoderDecoderFactory\",\n",
    ")\n",
    "\n",
    "# Create trainer and run 1 epoch\n",
    "# Note: Using CPU because MPS has issues with adaptive pooling on certain sizes\n",
    "trainer = Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"cpu\",  # Force CPU to avoid MPS pooling issues\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=False,\n",
    "    logger=False,  # Disable logging for this demo\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting training for 1 epoch...\")\n",
    "trainer.fit(task, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470cce5c",
   "metadata": {},
   "source": [
    "## Troubleshooting: What if a model URL changes?\n",
    "\n",
    "Sometimes HuggingFace or timm model URLs change (e.g., repos get renamed or moved).\n",
    "\n",
    "**Example:** Clay v1 models moved from `made-with-clay/Clay` to `made-with-clay/Clay-legacy`.\n",
    "\n",
    "Here's how to debug and find where URLs are configured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f89e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Loading a Clay model\n",
    "from terratorch.registry import BACKBONE_REGISTRY\n",
    "\n",
    "# Try loading Clay v1 backbone\n",
    "try:\n",
    "    clay_backbone = BACKBONE_REGISTRY.build(\"clay_v1_base\", pretrained=True)\n",
    "    print(\"‚úÖ Clay v1 loaded successfully!\")\n",
    "    print(f\"Type: {type(clay_backbone).__name__}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading Clay: {e}\")\n",
    "    print(\"\\nüîß If you see a 404 or connection error, the HuggingFace URL may have changed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce536b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to find and fix broken model URLs\n",
    "# \n",
    "# Step 1: Find where the model is defined\n",
    "#   - Search the codebase for the model name\n",
    "#   - For Clay: terratorch/models/backbones/clay_v1/embedder.py\n",
    "#\n",
    "# Step 2: Look for `default_cfgs` or `hf_hub_id`\n",
    "#   - This is where HuggingFace URLs are configured\n",
    "\n",
    "import inspect\n",
    "from terratorch.models.backbones.clay_v1 import embedder\n",
    "\n",
    "# Find the file location\n",
    "print(\"üìÅ FILE TO EDIT:\")\n",
    "print(f\"   {inspect.getfile(embedder)}\")\n",
    "print()\n",
    "\n",
    "# Show the current HuggingFace configuration\n",
    "print(\"üîó CURRENT HUGGINGFACE CONFIG:\")\n",
    "if hasattr(embedder, 'default_cfgs'):\n",
    "    for model_name, cfg in embedder.default_cfgs.items():\n",
    "        hf_url = getattr(cfg.default, 'hf_hub_id', 'Not set')\n",
    "        print(f\"   {model_name}: {hf_url}\")\n",
    "else:\n",
    "    print(\"   (Check the file manually for hf_hub_id or url settings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e1dba4",
   "metadata": {},
   "source": [
    "### Quick Reference: Where to fix model URLs\n",
    "\n",
    "| Model Family | Config File | Look For |\n",
    "|--------------|-------------|----------|\n",
    "| **Clay v1** | `terratorch/models/backbones/clay_v1/embedder.py` | `default_cfgs`, `hf_hub_id` |\n",
    "| **Prithvi** | `terratorch/models/backbones/prithvi_vit.py` | `default_cfgs`, `hf_hub_id` |\n",
    "| **ScaleMAE** | `terratorch/models/backbones/scalemae/scalemae.py` | `default_cfgs` |\n",
    "| **timm models** | timm library (external) | Update timm version |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttorch-env-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
